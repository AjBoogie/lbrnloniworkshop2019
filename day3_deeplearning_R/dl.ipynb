{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.1"
    },
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsuhpchelp/lbrnloniworkshop2019/blob/master/day3_deeplearning_R/dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3S7GYJhWMd",
        "colab_type": "text"
      },
      "source": [
        "Deep learning with R\n",
        "==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VfF3MZZbBdk",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "* Install and load R packages\n",
        "* `keras` package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exyDNnaoLyb9",
        "colab_type": "text"
      },
      "source": [
        "# Install and load R packages\n",
        "\n",
        "May take a while on the Colab\n",
        "\n",
        "R packages to be installed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqbPQILqLY3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages(\"keras\")\n",
        "install.packages(\"AmesHousing\")\n",
        "install.packages(\"rsample\")\n",
        "install.packages(\"yardstick\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZUJdm8fhW_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getwd()\n",
        "list.files()\n",
        "download.file(\"https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip\",\"mnist_csv.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rETLwifg3U5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list.files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmE_zcyZL6tE",
        "colab_type": "text"
      },
      "source": [
        "Load R packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHL7BY9kLxSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "library(keras)\n",
        "library(AmesHousing)\n",
        "library(rsample)\n",
        "library(yardstick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9PYryAIMThp",
        "colab_type": "text"
      },
      "source": [
        "# 1. `keras` pakcage\n",
        "## 1.1 A classification example: `MNIST` data\n",
        "* MNIST (Mixed National Institute of Standards and Technology\n",
        "database) is a large database of handwritten digits that is commonly\n",
        "used for training various image processing systems.\n",
        "* It consists of images of handwritten digits like these:\n",
        "\n",
        "> https://drive.google.com/open?id=1DnQzdwWLR4EnWuDteA8H9hgK6JOS3vmI\n",
        "\n",
        "* `MNIST` data is included in the `keras` package and can be accessed using the `dataset_mnist()` function, which has 60000 training images and 10000 testing images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goEezzOUL12m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist <- dataset_mnist()\n",
        "x_train <- mnist$train$x\n",
        "y_train <- mnist$train$y\n",
        "x_test <- mnist$test$x\n",
        "y_test <- mnist$test$y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih-K_JiiPxH6",
        "colab_type": "text"
      },
      "source": [
        "* Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers.\n",
        "\n",
        "* We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images.\n",
        "* Then, since the raw data has the grayscale values from integers ranging between 0 to 255, we convert the grayscale values into floating point values ranging between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmw_pb5xMLm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape\n",
        "x_train <- array_reshape(x_train, c(nrow(x_train), 784))\n",
        "x_test <- array_reshape(x_test, c(nrow(x_test), 784))\n",
        "# rescale\n",
        "x_train <- x_train / 255\n",
        "x_test <- x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SVEFEXhREW1",
        "colab_type": "text"
      },
      "source": [
        "* For the purposes of this tutorial, we label the y’s as \"one hot vectors\".\n",
        "* A one hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension.\n",
        "*  For example, how to label an “8”?\n",
        "> * \\[0, 0, 0, 0, 0, 0, 0, 1, 0, 0 ]\n",
        "* The one hot vector can be created using the `to_categorical()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYwFAkd4MUCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train <- to_categorical(y_train, 10)\n",
        "y_test <- to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWZwCUnFdMBW",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1 Defining the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2gPykENhaxe",
        "colab_type": "text"
      },
      "source": [
        "* A sequential model can be created by the `keras_model_sequential()` function then a series of layer functions.\n",
        "> * layers are added by using the pipe (`%>%`) operator, each layer is defined by the `layer_dense` function. (\"dense\" means full-connected) \n",
        "> * `units` positive integer, dimensionality of the output space (hidden nodes). The number of hidden nodes in a hidden layer is equal to or less than the number of features in the input but this is not a rule of thumb. \n",
        "> * `input_shape` dimensionality of the input (integer) **not** including the samples' axis (samples' axis is 60000 in this example). This argument is required when using this layer as the first layer in a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrTXLA8MglS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, input_shape = c(784)) %>% # hidden layer 1\n",
        "  layer_dense(units = 128) %>%                       # hidden layer 2\n",
        "  layer_dense(units = 10, activation = 'softmax')    # output layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Avi7aFrMlRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsEPXAF1uCZr",
        "colab_type": "text"
      },
      "source": [
        "## Quiz\n",
        "1. Why the total weights is 235146?\n",
        "2. Why the hidden nodes in the first hidden layer is set to 256? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CsBuD5Iuprd",
        "colab_type": "text"
      },
      "source": [
        "* Next, configure the model with appropriate loss function, optimizer, and metrics.\n",
        "> * `loss = 'categorical_crossentropy'` remember for classification we use cross-entropy\n",
        "> *  `optimizer = optimizer_rmsprop()` call an optimizer function \n",
        "> * For any classification problem you will want to set this to `metrics = c('accuracy')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WCyWijXMtY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTEIAu2YxEnZ",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.2 Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pf4Kj2zMxbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQYuZDzeTj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode --> we use model.matrix(...)[, -1] to discard the intercept\n",
        "data_onehot <- model.matrix(~ ., AmesHousing::make_ames())[, -1] %>% as.data.frame()\n",
        "\n",
        "# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.\n",
        "# Use set.seed for reproducibility\n",
        "set.seed(123)\n",
        "split <- rsample::initial_split(data_onehot, prop = .7, strata = \"Sale_Price\")\n",
        "train <- rsample::training(split)\n",
        "test  <- rsample::testing(split)\n",
        "\n",
        "# Create & standardize feature sets\n",
        "# training features\n",
        "train_x <- train %>% dplyr::select(-Sale_Price)\n",
        "mean    <- colMeans(train_x)\n",
        "std     <- apply(train_x, 2, sd)\n",
        "train_x <- scale(train_x, center = mean, scale = std)\n",
        "\n",
        "# testing features\n",
        "test_x <- test %>% dplyr::select(-Sale_Price)\n",
        "test_x <- scale(test_x, center = mean, scale = std)\n",
        "\n",
        "# Create & transform response sets\n",
        "train_y <- log(train$Sale_Price)\n",
        "test_y  <- log(test$Sale_Price)\n",
        "\n",
        "# zero variance variables (after one hot encoded) cause NaN so we need to remove\n",
        "zv <- which(colSums(is.na(train_x)) > 0, useNames = FALSE)\n",
        "train_x <- train_x[, -zv]\n",
        "test_x  <- test_x[, -zv]\n",
        "\n",
        "# What is the dimension of our feature matrix?\n",
        "dim(train_x)\n",
        "## [1] 2054  299\n",
        "dim(test_x)\n",
        "## [1] 876 299"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfrP1n35nzQ0",
        "colab_type": "text"
      },
      "source": [
        "Layers and nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkVw9GWFeyle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  layer_dense(units = 10, input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 5) %>%\n",
        "  layer_dense(units = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA_OzjEInoNa",
        "colab_type": "text"
      },
      "source": [
        "Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdq71my-fOAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  layer_dense(units = 10, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 5, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmbrKS_gndZM",
        "colab_type": "text"
      },
      "source": [
        "backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt3iNW2fdBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 10, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 5, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3c1-UCYnSt7",
        "colab_type": "text"
      },
      "source": [
        "Model training\n",
        "\n",
        "We’ve created a base model, now we just need to train it with our data. To do so we feed our model into a fit function along with our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfXPDNk2gG8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4IeRopahbI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=25)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 1.034\n",
        "##                val_loss: 4.078\n",
        "##     mean_absolute_error: 0.4967\n",
        "##                    loss: 0.5555\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2XfnGD_iNYT",
        "colab_type": "text"
      },
      "source": [
        "Overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T10OfaKh-xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 500, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 250, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 125, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE\n",
        ")\n",
        "\n",
        "plot(learn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDNMcKuhjXDt",
        "colab_type": "text"
      },
      "source": [
        "Adjust epochs\n",
        "\n",
        "Alternatively, if your epochs flatline early then there is no reason to run so many epochs as you are just using extra computational energy with no gain. We can add a callback function inside of fit to help with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPV0rxTEjYMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 50, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=6)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.8835\n",
        "##                val_loss: 1.353\n",
        "##     mean_absolute_error: 0.397\n",
        "##                    loss: 0.2517\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxP15fV1j_tL",
        "colab_type": "text"
      },
      "source": [
        "Add batch normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB0gfamMkHI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 50, activation = \"relu\") %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=25)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.2016\n",
        "##                val_loss: 0.07756\n",
        "##     mean_absolute_error: 0.1786\n",
        "##                    loss: 0.05482\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkjJxcYekUeK",
        "colab_type": "text"
      },
      "source": [
        "Add dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-phEhPmkX07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.2) %>%\n",
        "  layer_dense(units = 50, activation = \"relu\") %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.2) %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=19)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.4557\n",
        "##                val_loss: 0.4161\n",
        "##     mean_absolute_error: 1.185\n",
        "##                    loss: 2.247\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwSaoSZkydt",
        "colab_type": "text"
      },
      "source": [
        "Add weight regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn-EX2s3kzNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x),\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 50, activation = \"relu\",\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=18)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.5196\n",
        "##                val_loss: 0.6465\n",
        "##     mean_absolute_error: 0.342\n",
        "##                    loss: 0.3858\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_C2PcQplUpW",
        "colab_type": "text"
      },
      "source": [
        "Adjust learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVuGq-qTlZ2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x),\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 50, activation = \"relu\",\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2),\n",
        "    callback_reduce_lr_on_plateau()\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=25)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.2325\n",
        "##                val_loss: 0.2685\n",
        "##                      lr: 0.001\n",
        "##     mean_absolute_error: 0.2119\n",
        "##                    loss: 0.2482\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94i6s0aOlo2-",
        "colab_type": "text"
      },
      "source": [
        "Predicting\n",
        "We can use predict to predict our log sales prices for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhryhdp2lyzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% predict(test_x[1:10,])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XR_thghmFZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(results <- model %>% evaluate(test_x, test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP8no38tmPeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% \n",
        "  predict(test_x) %>% \n",
        "  broom::tidy() %>% \n",
        "  dplyr::mutate(\n",
        "    truth = test_y, \n",
        "    pred_tran = exp(x), \n",
        "    truth_tran = exp(truth)\n",
        "    ) %>%\n",
        "   yardstick::rmse(truth_tran, pred_tran)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxZ4qHp1gvus",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EchcJHLNWho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DluBDTHRNbU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8VC7mN4No5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% predict_classes(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVqGBwO-umJd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}