{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.1"
    },
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsuhpchelp/lbrnloniworkshop2019/blob/master/day3_deeplearning_R/dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3S7GYJhWMd",
        "colab_type": "text"
      },
      "source": [
        "Deep learning with R\n",
        "===\n",
        "\n",
        "![][logo]\n",
        "\n",
        "[logo](http://www.hpc.lsu.edu/training/weekly-materials/Downloads/Picture2.png \"Logo Title Text 1\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZUJdm8fhW_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getwd()\n",
        "list.files()\n",
        "download.file(\"https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip\",\"mnist_csv.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqbPQILqLY3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages(\"keras\")\n",
        "install.packages(\"AmesHousing\")\n",
        "install.packages(\"rsample\")\n",
        "install.packages(\"yardstick\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rETLwifg3U5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list.files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHL7BY9kLxSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "library(keras)\n",
        "library(AmesHousing)\n",
        "library(rsample)\n",
        "library(yardstick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQYuZDzeTj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode --> we use model.matrix(...)[, -1] to discard the intercept\n",
        "data_onehot <- model.matrix(~ ., AmesHousing::make_ames())[, -1] %>% as.data.frame()\n",
        "\n",
        "# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.\n",
        "# Use set.seed for reproducibility\n",
        "set.seed(123)\n",
        "split <- rsample::initial_split(data_onehot, prop = .7, strata = \"Sale_Price\")\n",
        "train <- rsample::training(split)\n",
        "test  <- rsample::testing(split)\n",
        "\n",
        "# Create & standardize feature sets\n",
        "# training features\n",
        "train_x <- train %>% dplyr::select(-Sale_Price)\n",
        "mean    <- colMeans(train_x)\n",
        "std     <- apply(train_x, 2, sd)\n",
        "train_x <- scale(train_x, center = mean, scale = std)\n",
        "\n",
        "# testing features\n",
        "test_x <- test %>% dplyr::select(-Sale_Price)\n",
        "test_x <- scale(test_x, center = mean, scale = std)\n",
        "\n",
        "# Create & transform response sets\n",
        "train_y <- log(train$Sale_Price)\n",
        "test_y  <- log(test$Sale_Price)\n",
        "\n",
        "# zero variance variables (after one hot encoded) cause NaN so we need to remove\n",
        "zv <- which(colSums(is.na(train_x)) > 0, useNames = FALSE)\n",
        "train_x <- train_x[, -zv]\n",
        "test_x  <- test_x[, -zv]\n",
        "\n",
        "# What is the dimension of our feature matrix?\n",
        "dim(train_x)\n",
        "## [1] 2054  299\n",
        "dim(test_x)\n",
        "## [1] 876 299"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfrP1n35nzQ0",
        "colab_type": "text"
      },
      "source": [
        "Layers and nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkVw9GWFeyle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  layer_dense(units = 10, input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 5) %>%\n",
        "  layer_dense(units = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA_OzjEInoNa",
        "colab_type": "text"
      },
      "source": [
        "Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdq71my-fOAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  layer_dense(units = 10, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 5, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmbrKS_gndZM",
        "colab_type": "text"
      },
      "source": [
        "backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt3iNW2fdBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 10, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 5, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3c1-UCYnSt7",
        "colab_type": "text"
      },
      "source": [
        "Model training\n",
        "\n",
        "We’ve created a base model, now we just need to train it with our data. To do so we feed our model into a fit function along with our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfXPDNk2gG8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4IeRopahbI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=25)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 1.034\n",
        "##                val_loss: 4.078\n",
        "##     mean_absolute_error: 0.4967\n",
        "##                    loss: 0.5555\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2XfnGD_iNYT",
        "colab_type": "text"
      },
      "source": [
        "Overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T10OfaKh-xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 500, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 250, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 125, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE\n",
        ")\n",
        "\n",
        "plot(learn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDNMcKuhjXDt",
        "colab_type": "text"
      },
      "source": [
        "Adjust epochs\n",
        "\n",
        "Alternatively, if your epochs flatline early then there is no reason to run so many epochs as you are just using extra computational energy with no gain. We can add a callback function inside of fit to help with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPV0rxTEjYMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_dense(units = 50, activation = \"relu\") %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=6)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.8835\n",
        "##                val_loss: 1.353\n",
        "##     mean_absolute_error: 0.397\n",
        "##                    loss: 0.2517\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxP15fV1j_tL",
        "colab_type": "text"
      },
      "source": [
        "Add batch normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB0gfamMkHI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 50, activation = \"relu\") %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=25)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.2016\n",
        "##                val_loss: 0.07756\n",
        "##     mean_absolute_error: 0.1786\n",
        "##                    loss: 0.05482\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkjJxcYekUeK",
        "colab_type": "text"
      },
      "source": [
        "Add dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-phEhPmkX07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.2) %>%\n",
        "  layer_dense(units = 50, activation = \"relu\") %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dropout(rate = 0.2) %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=19)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.4557\n",
        "##                val_loss: 0.4161\n",
        "##     mean_absolute_error: 1.185\n",
        "##                    loss: 2.247\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwSaoSZkydt",
        "colab_type": "text"
      },
      "source": [
        "Add weight regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn-EX2s3kzNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x),\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 50, activation = \"relu\",\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2)\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=18)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.5196\n",
        "##                val_loss: 0.6465\n",
        "##     mean_absolute_error: 0.342\n",
        "##                    loss: 0.3858\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_C2PcQplUpW",
        "colab_type": "text"
      },
      "source": [
        "Adjust learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVuGq-qTlZ2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "  \n",
        "  # network architecture\n",
        "  layer_dense(units = 100, activation = \"relu\", input_shape = ncol(train_x),\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 50, activation = \"relu\",\n",
        "              kernel_regularizer = regularizer_l2(0.001)) %>%\n",
        "  layer_batch_normalization() %>%\n",
        "  layer_dense(units = 1) %>%\n",
        "  \n",
        "  # backpropagation\n",
        "  compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"mse\",\n",
        "    metrics = c(\"mae\")\n",
        "  )\n",
        "\n",
        "# train our model\n",
        "learn <- model %>% fit(\n",
        "  x = train_x,\n",
        "  y = train_y,\n",
        "  epochs = 25,\n",
        "  batch_size = 32,\n",
        "  validation_split = .2,\n",
        "  verbose = FALSE,\n",
        "  callbacks = list(\n",
        "    callback_early_stopping(patience = 2),\n",
        "    callback_reduce_lr_on_plateau()\n",
        "  )\n",
        ")\n",
        "\n",
        "learn\n",
        "## Trained on 1,643 samples, validated on 411 samples (batch_size=32, epochs=25)\n",
        "## Final epoch (plot to see history):\n",
        "## val_mean_absolute_error: 0.2325\n",
        "##                val_loss: 0.2685\n",
        "##                      lr: 0.001\n",
        "##     mean_absolute_error: 0.2119\n",
        "##                    loss: 0.2482\n",
        "\n",
        "plot(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94i6s0aOlo2-",
        "colab_type": "text"
      },
      "source": [
        "Predicting\n",
        "We can use predict to predict our log sales prices for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhryhdp2lyzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% predict(test_x[1:10,])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XR_thghmFZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "a4bde48f-a175-4995-b300-e847b71ba01e"
      },
      "source": [
        "(results <- model %>% evaluate(test_x, test_y))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "$loss\n",
              "[1] 0.3253406\n",
              "\n",
              "$mean_absolute_error\n",
              "[1] 0.3080842\n"
            ],
            "text/latex": "\\begin{description}\n\\item[\\$loss] 0.325340595283465\n\\item[\\$mean\\_absolute\\_error] 0.308084189891815\n\\end{description}\n",
            "text/markdown": "$loss\n:   0.325340595283465\n$mean_absolute_error\n:   0.308084189891815\n\n\n",
            "text/html": [
              "<dl>\n",
              "\t<dt>$loss</dt>\n",
              "\t\t<dd>0.325340595283465</dd>\n",
              "\t<dt>$mean_absolute_error</dt>\n",
              "\t\t<dd>0.308084189891815</dd>\n",
              "</dl>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP8no38tmPeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fb6aa550-1997-4a1a-9e20-ae184787c8d5"
      },
      "source": [
        "model %>% \n",
        "  predict(test_x) %>% \n",
        "  broom::tidy() %>% \n",
        "  dplyr::mutate(\n",
        "    truth = test_y, \n",
        "    pred_tran = exp(x), \n",
        "    truth_tran = exp(truth)\n",
        "    ) %>%\n",
        "   yardstick::rmse(truth_tran, pred_tran)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message:\n",
            "“'tidy.matrix' is deprecated.\n",
            "See help(\"Deprecated\")”"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  .metric .estimator .estimate\n",
              "1 rmse    standard   95813.22 "
            ],
            "text/latex": "A tibble: 1 × 3\n\\begin{tabular}{r|lll}\n .metric & .estimator & .estimate\\\\\n <chr> & <chr> & <dbl>\\\\\n\\hline\n\t rmse & standard & 95813.22\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA tibble: 1 × 3\n\n| .metric &lt;chr&gt; | .estimator &lt;chr&gt; | .estimate &lt;dbl&gt; |\n|---|---|---|\n| rmse | standard | 95813.22 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A tibble: 1 × 3</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>.metric</th><th scope=col>.estimator</th><th scope=col>.estimate</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>rmse</td><td>standard</td><td>95813.22</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxZ4qHp1gvus",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ6VwPFqX7ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install_keras(tensorflow = \"gpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goEezzOUL12m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist <- dataset_mnist()\n",
        "x_train <- mnist$train$x\n",
        "y_train <- mnist$train$y\n",
        "x_test <- mnist$test$x\n",
        "y_test <- mnist$test$y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmw_pb5xMLm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train <- array_reshape(x_train, c(nrow(x_train), 784))\n",
        "x_test <- array_reshape(x_test, c(nrow(x_test), 784))\n",
        "# rescale\n",
        "x_train <- x_train / 255\n",
        "x_test <- x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYwFAkd4MUCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train <- to_categorical(y_train, 10)\n",
        "y_test <- to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrTXLA8MglS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model <- keras_model_sequential() \n",
        "model %>% \n",
        "  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \n",
        "  layer_dropout(rate = 0.4) %>% \n",
        "  layer_dense(units = 128, activation = 'relu') %>%\n",
        "  layer_dropout(rate = 0.3) %>%\n",
        "  layer_dense(units = 10, activation = 'softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Avi7aFrMlRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WCyWijXMtY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% compile(\n",
        "  loss = 'categorical_crossentropy',\n",
        "  optimizer = optimizer_rmsprop(),\n",
        "  metrics = c('accuracy')\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pf4Kj2zMxbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history <- model %>% fit(\n",
        "  x_train, y_train, \n",
        "  epochs = 30, batch_size = 128, \n",
        "  validation_split = 0.2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EchcJHLNWho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DluBDTHRNbU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8VC7mN4No5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model %>% predict_classes(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}